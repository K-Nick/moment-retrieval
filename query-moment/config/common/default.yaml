_base_:
  - "./runtime.yaml"

pipeline_verbose: ${debug}

data:
  dataset: tacos
  dataset_dir: ${data_dir}/${data.dataset}
  vid_feat_type: clip.video.avg.avg
  # upload_vocab_key: glove_vocab
  num_clip: 256

  pretrained: openai/clip-vit-large-patch14-336

  pre_processors:
    # - type: glove_tokenize
    #   vocab_file: ${data.dataset_dir}/annot/vocab.txt
    #   cache_dir: ${cache_dir}/.glove
    #   upload_vocab_key: ${data.upload_vocab_key}
    #   from_key: text
    #   to_indices: true
    - type: hf_embedding
      model_cls: CLIPTextModel
      extractor_cls: CLIPTokenizer
      from_key: text
      pretrained: ${data.pretrained}
      cache_args:
        hash_key: sample_id
        cache_dir: ${cache_dir}/${data.dataset}/${data.pretrained}
        verbose: true
        load_to_memory: true

  processors:
    - type: load_hdf5
      hdf5_file: ${data.dataset_dir}/${data.vid_feat_type}.hdf5
      path_template: "{}/patch"
      from_key: video_id
    - type: seq_sample
      axis: 0
      max_len: ${data.num_clip}
      from_key: video_id_hdf5
    - type: batch.seq_pad
      axis: 0
      fill_value: 0.0
      from_key: video_id_hdf5_sample
    - type: batch.seq_pad
      axis: 0
      fill_value: 0.0
      from_key: text_embeddings
    - type: rename
      from_keys:
        [
          video_id_hdf5_sample_pad,
          video_id_hdf5_sample_mask,
          text_embeddings_pad,
          text_embeddings_mask,
        ]
      to_keys: [vid_feat, vid_mask, text_feat, text_mask]
    - type: collect
      from_keys: [vid_feat, vid_mask, text_feat, text_mask, gt]

  collater: default

train:
  num_epoch: 12
  eval_epoch_interval: 1
  batch_size: 16
  prefetch_factor: 2
  num_workers: 0

  optimizer:
    type: adam_w
    lr: 1e-5
    weight_decay: 0.98
