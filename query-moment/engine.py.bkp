import torch
import torch.nn as nn
import pytorch_lightning as pl
from pytorch_lightning.utilities.seed import seed_everything
from detectron2.config import instantiate
from kn_util.config import dispatch_arguments_to_cfgs
from misc import calc_iou_score_gt
from einops import repeat
from kn_util.general import registry, get_logger
import pandas as pd
import wandb
import copy

log = get_logger(__name__)


from torchmetrics import Metric
class AverageMeter(Metric):
    higher_is_better = False
    full_state_update = True

    def __init__(self):
        super().__init__()
        self.add_state("sum", default=torch.tensor(0.0), dist_reduce_fx="sum")
        self.add_state("n", default=torch.tensor(0.0), dist_reduce_fx="sum")

    def update(self, val):
        self.sum += val
        self.n += 1

    def compute(self):
        return self.sum / self.n


class RankMIoUAboveN(Metric):
    higher_is_better = True
    full_state_update = True

    def __init__(self, m, n) -> None:
        super().__init__()
        self.m = m
        self.n = n
        self.add_state("hit", default=torch.tensor(0.0), dist_reduce_fx="sum")
        self.add_state("num_sample", default=torch.tensor(0.0), dist_reduce_fx="sum")

    def update(self, pred_bds, scores, gt):
        B = len(pred_bds)
        pred_bds_batch = pred_bds
        gt_batch = gt
        scores_batch = scores

        for i in range(B):
            pred_bds = pred_bds_batch[i]
            gt = gt_batch[i]
            scores = scores_batch[i]

            _, sorted_index = torch.sort(scores)
            pred_bds = pred_bds[sorted_index][:self.m]

            Nc, _2 = pred_bds.shape
            expand_gt = repeat(gt, "i -> nc i", nc=Nc)
            ious = calc_iou_score_gt(pred_bds, expand_gt)
            is_hit = torch.sum(ious >= self.n)
            self.hit += (is_hit > 0).float()
            self.num_sample += 1

    def compute(self):
        return self.hit / self.num_sample


class MomentRetrievalModule(pl.LightningModule):

    def __init__(self, cfg) -> None:
        super().__init__()
        self.save_hyperparameters()
        self.cfg = cfg
        self.net = instantiate(cfg.model)
        metrics = self.build_metrics()
        self.metrics = dict(_val=metrics["val"], _test=metrics["test"])

    def build_metrics(self):
        cfg = self.cfg
        metrics = dict()
        # metrics["train"] = {k: AverageMeter() for k in cfg.train.loss_keys}
        # build from runtime
        metrics["val"] = dict()
        metrics["test"] = dict()
        for m in [1, 5]:
            for n in [0.3, 0.5, 0.7]:
                name = f"Rank{m}@IoU={n:.1f}".replace(".", "")
                metrics["val"][name] = RankMIoUAboveN(m, n)
                metrics["test"][name] = RankMIoUAboveN(m, n)
        return metrics

    def update_metric(self, outputs, domain):
        if domain == "train":
            if "_train" not in self.metrics:
                self.metrics["_train"] = nn.ModuleDict({k: AverageMeter() for k in outputs})
            metrics = self.metrics["_train"]
            for k in outputs:
                metrics[k].update(outputs[k].item())
        else:
            metrics = self.metrics["_" + domain]
            for metric in metrics.values():
                if isinstance(metric, RankMIoUAboveN):
                    metric.update(outputs["boxxes"].cpu(), outputs["score"].cpu(), outputs["gt"].cpu())

    def log_metric(self, domain):
        metrics = self.metrics["_" + domain]
        ret_dict = dict()

        for name, metric in metrics.items():
            val = metric.compute().item()
            ret_dict[f"{domain}/{name}"] = val
            self.log(f"{domain}/{name}", val)

            metric.reset()
        log.info(f"\n==============={domain} result===============")
        log.info("\n" + pd.Series(ret_dict).to_string())

    def forward(self, *args, **kwargs):
        return self.net(*args, **kwargs)

    def on_train_epoch_start(self):
        seed = self.cfg.flags.seed
        seed_everything(seed + self.current_epoch)

    def training_step(self, batch, batch_idx):
        losses = self.net(**batch, mode="train")
        """<DEBUG> output_grad"""
        from kn_util.general import registry
        from kn_util.debug import explore_content as EC
        if registry.get_object("output_grad", False):
            losses['loss'].backward()
            for name, param in self.net.named_parameters():
                if param.grad is not None:
                    print(name, param.grad.norm())
                else:
                    print(name)
            import ipdb
            ipdb.set_trace()  #FIXME
        """<DEBUG>"""

        return losses

    def training_step_end(self, losses):
        self.update_metric(losses, "train")

    def training_epoch_end(self, outputs):
        self.log_metric("train")

    def validation_step(self, batch, batch_idx):
        with torch.no_grad():
            infer_outputs = self.net(**batch, mode="inference")
        infer_outputs.update(batch)
        self.update_metric(infer_outputs, "val")

        return infer_outputs

    def validation_epoch_end(self, outputs):
        self.log_metric("val")

    def test_step(self, batch, batch_idx):
        with torch.no_grad():
            infer_outputs = self.net(**batch, mode="inference")
        infer_outputs.update(batch)
        self.update_metric(infer_outputs, "test")

        return infer_outputs

    def test_epoch_end(self, outputs) -> None:
        self.log_metric("test")

    def configure_optimizers(self):
        optimizer_cfg = copy.deepcopy(self.cfg.train.optimizer)
        optimizer_cfg.params = self.net.parameters()
        optimizer = instantiate(optimizer_cfg)
        return optimizer


# def train_one_epoch(model, dataloader, optimizer):
#     pass